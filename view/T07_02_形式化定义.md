# T07 AI 内部机制：形式化定义

**核心论点**：将注意力机制、信息流动与隐空间表示用数学形式化表述，为机制分析、可视化和工程调试提供统一语言。

**创建日期**：2025-01-30
**最后更新**：2025-01-30
**关联主题**：T07 [AI内部机制_机制分析](T07_AI内部机制_机制分析.md)、[T07_01_理论基础](T07_01_理论基础.md)

---

## 一、概述

本文档给出 T07（AI 内部机制）的形式化定义：自注意力与多头注意力、前向/反向传播与梯度计算、隐空间与嵌入空间的数学表述，与 T07_01 理论基础、T07_03/04 实证与工程实践对齐。

---

## 二、目录

- [T07 AI 内部机制：形式化定义](#t07-ai-内部机制形式化定义)
  - [一、概述](#一概述)
  - [二、目录](#二目录)
  - [三、注意力机制形式化](#三注意力机制形式化)
    - [3.1 单头自注意力](#31-单头自注意力)
    - [3.2 多头注意力](#32-多头注意力)
    - [3.3 位置编码](#33-位置编码)
  - [四、信息流动形式化](#四信息流动形式化)
    - [4.1 前向传播](#41-前向传播)
    - [4.2 反向传播与梯度](#42-反向传播与梯度)
    - [4.3 残差连接的梯度](#43-残差连接的梯度)
  - [五、隐空间表示形式化](#五隐空间表示形式化)
    - [5.1 向量空间与嵌入](#51-向量空间与嵌入)
    - [5.2 层表示与流形](#52-层表示与流形)
    - [5.3 注意力权重矩阵](#53-注意力权重矩阵)
  - [六、结论](#六结论)
  - [参考文献](#参考文献)

---

## 三、注意力机制形式化

### 3.1 单头自注意力

设输入序列 \( X \in \mathbb{R}^{n \times d} \)，\( W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k} \)（或 \( d \times d \)），则：

\[
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\]

其中 \( Q = X W_Q,\ K = X W_K,\ V = X W_V \)。\( \frac{QK^\top}{\sqrt{d_k}} \) 为注意力得分，softmax 后为权重，对 \( V \) 加权求和得到每个位置的输出。

### 3.2 多头注意力

\( h \) 个头，每头维度 \( d_k = d/h \)：

\[
\mathrm{MultiHead}(X) = \mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h) W^O
\]

\[
\mathrm{head}_i = \mathrm{Attention}(X W_Q^i,\, X W_K^i,\, X W_V^i)
\]

\( W^O \in \mathbb{R}^{h d_k \times d} \)。多头允许不同子空间关注不同模式。

### 3.3 位置编码

- **正弦编码**：\( \mathrm{PE}_{(pos,2i)} = \sin(pos/10000^{2i/d}) \)，\( \mathrm{PE}_{(pos,2i+1)} = \cos(pos/10000^{2i/d}) \)。
- **可学习**：\( \mathrm{PE} \in \mathbb{R}^{n_{\max} \times d} \) 为参数。输入为 \( X + \mathrm{PE} \) 或 \( X + \mathrm{PE}_{:n} \)。

---

## 四、信息流动形式化

### 4.1 前向传播

设第 \( \ell \) 层参数 \( \theta^{(\ell)} \)，激活 \( a^{(\ell)} = f^{(\ell)}(a^{(\ell-1)}; \theta^{(\ell)}) \)，则前向为：

\[
a^{(0)} = x,\quad a^{(\ell)} = f^{(\ell)}(a^{(\ell-1)}; \theta^{(\ell)}),\ \ell=1,\ldots,L.
\]

输出 \( \hat{y} = g(a^{(L)}) \)，损失 \( \mathcal{L} = \ell(\hat{y}, y) \)。

### 4.2 反向传播与梯度

\[
\frac{\partial \mathcal{L}}{\partial \theta^{(\ell)}} = \frac{\partial \mathcal{L}}{\partial a^{(L)}} \cdot \prod_{k=\ell+1}^{L} \frac{\partial a^{(k)}}{\partial a^{(k-1)}} \cdot \frac{\partial a^{(\ell)}}{\partial \theta^{(\ell)}}.
\]

链式因子中若 \( \left\| \frac{\partial a^{(k)}}{\partial a^{(k-1)}} \right\| \ll 1 \) 则梯度消失；若 \( \gg 1 \) 则梯度爆炸。

### 4.3 残差连接的梯度

\( a^{(\ell)} = a^{(\ell-1)} + F(a^{(\ell-1)}; \theta^{(\ell)}) \) 时，\( \frac{\partial a^{(\ell)}}{\partial a^{(\ell-1)}} \) 含单位项 1，梯度至少有一条不衰减路径。

---

## 五、隐空间表示形式化

### 5.1 向量空间与嵌入

词表 \( \mathcal{V} \)，嵌入 \( E \in \mathbb{R}^{|\mathcal{V}| \times d} \)。词 \( w \) 的 one-hot \( e_w \)，表示 \( x = E^\top e_w \in \mathbb{R}^d \)。序列表示为 \( X \in \mathbb{R}^{n \times d} \)。

### 5.2 层表示与流形

第 \( \ell \) 层输出 \( H^{(\ell)} \in \mathbb{R}^{n \times d} \) 视为该层“隐表示”。在适当正则与数据分布下，表示可被视为低维流形上的点；几何与距离用于解释语义与机制行为。

### 5.3 注意力权重矩阵

\( A = \mathrm{softmax}(QK^\top/\sqrt{d_k}) \in \mathbb{R}^{n \times n} \) 为位置间依赖的显式形式化，是机制可视化的主要对象（注意力头、层、位置维度）。

---

## 六、结论

- 注意力、前向/反向与隐空间的形式化给出了可计算、可测量的“机制”定义。
- 与 T07_01 理论、T07_03 实证、T07_04 工程工具一致，支撑可解释性与调试。

---

## 参考文献

1. A. Vaswani et al., "Attention is all you need," in Proc. NeurIPS, 2017, pp. 5998–6008.
2. K. He et al., "Deep residual learning for image recognition," in Proc. CVPR, 2016, pp. 770–778.

---

**文档版本**：1.0
