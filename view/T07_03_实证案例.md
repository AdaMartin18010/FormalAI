# T07 AI 内部机制：实证案例

**核心论点**：通过注意力可视化、内部表示分析与机制缺陷诊断的实证案例，说明“机制”可观测、可解释，并支撑工程改进。

**创建日期**：2025-01-30
**最后更新**：2025-01-30
**关联主题**：T07 [AI内部机制_机制分析](T07_AI内部机制_机制分析.md)、[T07_02_形式化定义](T07_02_形式化定义.md)

---

## 一、概述

本文档给出 T07 的实证案例：BERT/GPT-2 与 ViT 的注意力模式、隐空间与层级表示分析、注意力崩溃与梯度问题等机制缺陷诊断，与理论基础和形式化定义对应，为 T07_04 工程实践提供依据。

---

## 二、目录

- [一、概述](#一概述)
- [二、目录](#二目录)
- [三、注意力可视化案例](#三注意力可视化案例)
- [四、内部表示分析案例](#四内部表示分析案例)
- [五、机制缺陷诊断案例](#五机制缺陷诊断案例)
- [六、结论](#六结论)
- [参考文献](#参考文献)

---

## 三、注意力可视化案例

### 3.1 BERT 注意力模式

- **现象**：低层更多局部语法（相邻词、短语），高层更多语义与共指；部分头专注 [CLS]、[SEP] 或标点。
- **方法**：提取各层各头 \( A = \mathrm{softmax}(QK^\top/\sqrt{d_k}) \)，对序列维度或头维度聚合后可视化（热力图、图）。
- **工具**：BertViz、Transformers 的 `attentions` 输出、自定义 hook。

### 3.2 GPT-2 注意力模式

- **因果掩码**：上三角被掩，仅能看到左侧上下文；可视化可展示“当前词关注哪些前文位置”。
- **现象**：重复与循环模式、长程依赖的稀疏头、部分头近似“局部窗口”。

### 3.3 Vision Transformer (ViT) 注意力可视化

- **Patch 级**：将 patch 序列的注意力矩阵映射回 2D 网格，得到“哪些 patch 互相关注”；可与物体边界、语义区域对应。
- **跨层**：低层更局部/边缘，高层更全局/语义，与 CNN 层级解释可类比。

---

## 四、内部表示分析案例

### 4.1 隐空间表示可视化

- **方法**：取某层 \( H^{(\ell)} \in \mathbb{R}^{n \times d} \)，对 \( d \) 维做 PCA/t-SNE/UMAP 降至 2D 或 3D；按词性、实体、语义类别着色。
- **发现**：同义/同类词聚类、句法结构在表示空间中的几何关系、层越深语义越主导。

### 4.2 语义空间结构

- **探针**：用线性分类器或简单 MLP 从 \( H^{(\ell)} \) 预测句法标签（如成分、依赖）；准确率随层升高可解释为“语法信息在高层被语义吸收或重组”。
- **相似度**：余弦或欧氏距离在表示空间中的分布，用于解释检索、类比与泛化。

### 4.3 层级表示演化

- **层间相关与 CKA**：比较不同层的表示相似度（CKA、SVCCA）；可发现跳跃连接、瓶颈层对表示演化的影响。
- **任务相关性**：各层对下游任务（分类、QA、生成）的贡献（冻结/微调、层丢弃实验）。

---

## 五、机制缺陷诊断案例

### 5.1 注意力崩溃

- **现象**：注意力分布极不均匀（近似 one-hot 或均匀），信息利用不足。
- **诊断**：熵、max 权重、有效头数；与性能下降、重复生成、长序列退化相关。
- **缓解**：初始化、温度、正则（如 entropy 约束）、架构（如 ALiBi、局部注意力）。

### 5.2 梯度消失/爆炸

- **现象**：训练不稳定、底层梯度接近 0 或数值溢出。
- **诊断**：梯度范数随层/步数曲线、梯度直方图；与深度、激活、初始化、学习率相关。
- **缓解**：残差、LayerNorm、梯度裁剪、更好的初始化与学习率调度。

### 5.3 过拟合与表示退化

- **现象**：训练损失持续下降而验证变差；表示协方差秩塌缩、样本在表示空间过于接近。
- **诊断**：表示秩、类内/类间距离、特征冗余度。
- **缓解**：正则、数据增强、Dropout、早停、表示正则化（如 Barlow Twin 思想）。

---

## 六、结论

- 注意力可视化与内部表示分析将“机制”变为可测量对象；机制缺陷诊断将理论与工程问题衔接。
- 与 T07_01/02 理论形式化、T07_04 工具与调试流程一致，共同支撑 T07 主题完整性。

---

## 参考文献

1. J. Vig, "A multiscale visualization of attention in the transformer model," ACL Demo, 2019.
2. E. Clark et al., "What does BERT look at? An analysis of BERT's attention," in Proc. BlackboxNLP, 2019.
3. N. C. Thompson et al., "The computational limits of deep learning," arXiv:2007.05558, 2020.

---

**文档版本**：1.0
