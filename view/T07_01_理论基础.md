# T07 AI 内部机制：理论基础

**核心论点**：AI 内部机制的理论基础来自认知科学（认知架构、信息处理、注意力）与深度学习（前向/反向传播、Transformer 自注意力、位置编码与残差），二者共同支撑对“机制”的界定与可解释性分析。

**创建日期**：2025-01-30
**最后更新**：2025-01-30
**关联主题**：T07 [AI内部机制_机制分析](T07_AI内部机制_机制分析.md)

---

## 一、概述

本文档为 VIEW 主题 T07（AI 内部机制）的理论基础部分，涵盖：认知科学中的认知架构与信息处理模型、神经网络中的前向/反向传播与梯度流动、Transformer 的自注意力与位置编码及残差连接。为 T07_02 形式化定义与 T07_03/04 实证与工程实践提供概念与理论依据。

---

## 二、目录

- [一、概述](#一概述)
- [二、目录](#二目录)
- [三、认知科学理论基础](#三认知科学理论基础)
- [四、神经网络理论基础](#四神经网络理论基础)
- [五、Transformer 理论基础](#五transformer-理论基础)
- [六、结论](#六结论)
- [参考文献](#参考文献)

---

## 三、认知科学理论基础

### 3.1 认知架构理论（ACT-R、SOAR、CLARION）

- **ACT-R**：适应性思维控制架构；将认知分解为模块（视觉、运动、记忆等）与缓冲器，生产规则驱动“条件-动作”序列；与深度学习中的“条件计算”“专家路由”有概念对应。
- **SOAR**：状态-算子-结果；统一子目标与学习（块化），强调状态空间搜索；与强化学习中的 MDP 与策略-价值分解有对应。
- **CLARION**：显式/隐式双系统；底层联结主义、顶层符号规则；为“系统 1/系统 2”及神经符号融合提供理论参照。

### 3.2 信息处理模型（Atkinson–Shiffrin）

- 感觉登记 → 短时记忆（容量有限、复述维持）→ 长时记忆（语义编码）；与深度学习中的**短期上下文（上下文窗口）**与**长期记忆（检索、压缩、更新、遗忘）**可类比，但不做等同主张。

### 3.3 认知科学中的注意力机制

- 选择性注意、资源有限、瓶颈模型；与神经网络中的**注意力权重**（对输入的加权聚合）在数学上对应为“软选择”，在功能上对应“信息路由与聚焦”。

---

## 四、神经网络理论基础

### 4.1 前向传播机制

- 输入经线性变换与非线性激活逐层传递；每层输出作为下一层输入；最终得到损失（如交叉熵、MSE）。前向传播定义了“表示”的逐层变换路径。

### 4.2 反向传播机制

- 损失对参数的梯度沿计算图反向传播（链式法则）；用于更新权重（如 SGD、Adam）。反向传播是“机制可分析”的基础（梯度流、梯度消失/爆炸）。

### 4.3 梯度流动理论

- 梯度范数随深度衰减或爆炸；残差连接、归一化（LayerNorm）、初始化与学习率调度共同影响梯度流动，进而影响可训练性与内部表示的稳定性。

---

## 五、Transformer 理论基础

### 5.1 自注意力机制理论（Vaswani et al. 2017）

- **形式化**［Vaswani et al., NeurIPS 2017］：查询 Q、键 K、值 V；标量注意力得分为 \( \mathit{score}(Q,K) = QK^\top/\sqrt{d_k} \)，经 softmax 得权重，对 V 加权求和：\( \mathrm{Attention}(Q,K,V) = \mathrm{softmax}(QK^\top/\sqrt{d_k})V \)。多头为 \( \mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^O \)，其中 \( \mathrm{head}_i = \mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V) \)。
- 自注意力实现序列内任意位置的信息聚合，是“内部机制”的核心可观测对象；\( \sqrt{d_k} \) 缩放用于稳定梯度。

### 5.2 位置编码理论

- 绝对位置（正弦/可学习）或相对位置（RoPE、ALiBi）为序列注入顺序信息；影响注意力模式与长程依赖，属于机制设计的一部分。

### 5.3 残差连接理论

- 输出 = 子层输出 + 输入；缓解梯度消失、稳定训练、支持深层堆叠；与“信息直通路径”的机制解释一致。

---

## 六、结论

- 认知科学提供“机制”的认知层面参照（架构、记忆、注意力）；神经网络与 Transformer 提供可计算、可观测的机制实现。
- 三者共同构成 T07 的理论基础，为形式化定义（T07_02）、实证案例（T07_03）与工程实践（T07_04）提供一致的概念框架。

---

## 参考文献

1. J. R. Anderson, *Cognitive Psychology and Its Implications*, 8th ed. New York: Worth, 2020.
2. A. Vaswani et al., "Attention is all you need," in *Advances in Neural Information Processing Systems* (NeurIPS), vol. 30, 2017, pp. 5998–6008.
3. D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning representations by back-propagating errors," *Nature*, vol. 323, no. 6088, pp. 533–536, 1986.

---

**文档版本**：1.1
**最后更新**：2025-01-30（修正 Vaswani 引用，补充自注意力与多头形式化）
